# -*- coding: utf-8 -*-
"""Mercedes project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rFY9Ju3PNfktMFIN_AH3brn-jDuWidap

Mercedes-Benz Greener Manufacturing
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.decomposition import PCA
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV

"""importing dataset"""

train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')

train.shape

test.shape

train

test

"""1) If for any column(s), the variance is equal to zero, then you need to remove those variable(s)."""

var_df=pd.DataFrame(np.var(train,0),columns=['variance'])
var_df.sort_values(by='variance')

col=list((var_df[var_df['variance']==0]).index)
print('columns with variance 0',col)

train.drop(col,axis=1,inplace=True)

train.shape

"""2) Check for null and unique values for test and train sets."""

value1=train.isna().sum().value_counts()
print('missing value in train dataset =', value1)

print('Number of unique values in train dataset =',train.nunique())

value2=test.isna().sum().value_counts()
print('missing value in test dataset =', value2)

print('Number of unique values in test dataset=',test.nunique())

"""3) Apply label encoder."""

object_col=[]
for i in train.columns:
    a= train[i].dtype
    if a =='object':
        object_col.append(i)
print(object_col)

for i in object_col:
    print(i, train[i].nunique())

from sklearn.preprocessing import LabelEncoder
LE=LabelEncoder()
for i in object_col:
    train[i]=LE.fit_transform(train[i])

train.info()

"""4) Perform dimensionality reduction."""

?PCA

train

xdata=train.drop('y',axis=1)
xdata.shape

ydata=train['y']
ydata.shape

PCA1=PCA(random_state=0)
Principal_comp=PCA1.fit(xdata)

plt.plot(np.cumsum(PCA1.explained_variance_))
plt.title("Mercedes Dataset Explained variance")
plt.xlabel("Number of Components")
plt.ylabel("Variance")
plt.xlim(0,15)
plt.show()

PCA1=PCA(n_components=5)
Principal_data=PCA1.fit_transform(xdata)
PCA1

PCA1.explained_variance_ratio_

PCA1.explained_variance_ratio_.sum()

"""5) Predict your test_df values using XGBoost."""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(xdata,ydata,test_size=0.25,random_state=50)

XGboost=XGBRegressor()
xgbparams={'max_depth': [3,5,10,15], 'learning_rate' : [0.1], 'n_estimators' : [50,100,200,300],'random_state' :[0]}
grid=zip([XGboost],[xgbparams])
best_model=None
for model,param in grid:
    temp=GridSearchCV(model,param_grid=param,cv=3,n_jobs=1)
    temp.fit(X_train,Y_train)
    if best_model is None:
        best_model=temp
    else:
        if temp.best_score_ > best_model.best_score_:
            best_model = temp
print ("Best CV Score",best_model.best_score_)
print ("Model Parameters",best_model.best_params_)
print("Best Estimator",best_model.best_estimator_)

model=best_model.best_estimator_
model.fit(X_train,Y_train)
pred=model.predict(X_test)

from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
r2_score(Y_test,pred)

print(model.score(X_train,Y_train))
print(model.score(X_test,Y_test))

"""Conclusion :
1)there are total 12 columns with variance 0 that are removed.
2)there is no missing value in our train and test dataset.
3)Applied label encoder.
4)Performed dimensionality reduction using PCA get 94% variance for 5 component.
5) After applying Xgboost regressor algorithm, get accuracy of 60% and it can be used for test prediction
"""